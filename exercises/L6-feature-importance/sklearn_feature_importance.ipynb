{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance method in sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get a sense of how feature importance is calculated in sci-kit learn, and also see where it gives results that we wouldn't expect.\n",
    "\n",
    "Sci-kit learn uses gini impurity to calculate a measure of impurity for each node.  Gini impurity, like entropy is a way to measure how \"disorganized\" the observations are before and after splitting them using a feature. So there is an impurity measure for each node.\n",
    "\n",
    "In the formula, freq_{i} is the frequency of label \"i\".  C is the number of unique labels at that node.\n",
    "\n",
    "$Impurity= \\sum_{i=1}^{C} - freq_{i} * (1- freq_{i})$\n",
    "\n",
    "The node importance in sci-kit learn is calculated as the difference between the gini impurity of the node and the gini impurity of its left and right children.  These gini impurities are weighted by the number of data points that reach each node.\n",
    "\n",
    "$NodeImportance = w_{i} Impurity_{i} - ( w_{left} Impurity_{left} + w_{right} Impurity_{right} )$\n",
    "\n",
    "The importance of a feature is the importance of the node that it was split on, divided by the sum of all node importances in the tree.  You’ll get to practice this in the coding exercise coming up next!\n",
    "\n",
    "For additional reading, please check out this blog post [The Mathematics of Decision Trees, Random Forest and Feature Importance in Scikit-learn and Spark](https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "We'll generate features and labels that form the \"AND\" operator.  So when feature 0 and feature 1 are both 1, then the label is 1, else the label is 0.  The third feature, feature 2, won't have an effect on the label output (it's always zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features 0 and 1 form the AND operator\n",
    "Feature 2 is always zero.\n",
    "\"\"\"\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the labels\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the trained decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"301pt\" height=\"266pt\"\n",
       " viewBox=\"0.00 0.00 300.50 265.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 261.5)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-261.5 296.5,-261.5 296.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#eeab7b\" stroke=\"black\" d=\"M159.62,-257.5C159.62,-257.5 70.88,-257.5 70.88,-257.5 64.88,-257.5 58.88,-251.5 58.88,-245.5 58.88,-245.5 58.88,-205.5 58.88,-205.5 58.88,-199.5 64.88,-193.5 70.88,-193.5 70.88,-193.5 159.62,-193.5 159.62,-193.5 165.62,-193.5 171.62,-199.5 171.62,-205.5 171.62,-205.5 171.62,-245.5 171.62,-245.5 171.62,-251.5 165.62,-257.5 159.62,-257.5\"/>\n",
       "<text text-anchor=\"start\" x=\"90.88\" y=\"-241.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"97.62\" y=\"-241.2\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\n",
       "<text text-anchor=\"start\" x=\"104.38\" y=\"-241.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"78.12\" y=\"-227.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.375</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-213.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"66.88\" y=\"-199.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [75, 25]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M92.5,-154.25C92.5,-154.25 12,-154.25 12,-154.25 6,-154.25 0,-148.25 0,-142.25 0,-142.25 0,-108.75 0,-108.75 0,-102.75 6,-96.75 12,-96.75 12,-96.75 92.5,-96.75 92.5,-96.75 98.5,-96.75 104.5,-102.75 104.5,-108.75 104.5,-108.75 104.5,-142.25 104.5,-142.25 104.5,-148.25 98.5,-154.25 92.5,-154.25\"/>\n",
       "<text text-anchor=\"start\" x=\"23.38\" y=\"-136.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"9.12\" y=\"-120.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-103.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.14,-193.22C89.08,-183.8 82.38,-173.36 76.1,-163.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.6,-162.02 70.25,-155.5 72.71,-165.8 78.6,-162.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"64.94\" y=\"-173.88\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M223.62,-157.5C223.62,-157.5 134.88,-157.5 134.88,-157.5 128.88,-157.5 122.88,-151.5 122.88,-145.5 122.88,-145.5 122.88,-105.5 122.88,-105.5 122.88,-99.5 128.88,-93.5 134.88,-93.5 134.88,-93.5 223.62,-93.5 223.62,-93.5 229.62,-93.5 235.62,-99.5 235.62,-105.5 235.62,-105.5 235.62,-145.5 235.62,-145.5 235.62,-151.5 229.62,-157.5 223.62,-157.5\"/>\n",
       "<text text-anchor=\"start\" x=\"154.88\" y=\"-141.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"161.62\" y=\"-141.2\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"168.38\" y=\"-141.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"150.38\" y=\"-127.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"136.12\" y=\"-113.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"130.88\" y=\"-99.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [25, 25]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.68,-193.22C141.16,-184.82 147.17,-175.62 152.92,-166.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.26,-169.1 158.8,-158.82 150.4,-165.28 156.26,-169.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.93\" y=\"-177.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M157.5,-57.5C157.5,-57.5 77,-57.5 77,-57.5 71,-57.5 65,-51.5 65,-45.5 65,-45.5 65,-12 65,-12 65,-6 71,0 77,0 77,0 157.5,0 157.5,0 163.5,0 169.5,-6 169.5,-12 169.5,-12 169.5,-45.5 169.5,-45.5 169.5,-51.5 163.5,-57.5 157.5,-57.5\"/>\n",
       "<text text-anchor=\"start\" x=\"88.38\" y=\"-40.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"74.12\" y=\"-23.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [25, 0]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.78,-93.21C153.29,-84.82 147.29,-75.66 141.61,-66.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.02,-65.26 135.61,-58.81 138.16,-69.1 144.02,-65.26\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M280.5,-57.5C280.5,-57.5 200,-57.5 200,-57.5 194,-57.5 188,-51.5 188,-45.5 188,-45.5 188,-12 188,-12 188,-6 194,0 200,0 200,0 280.5,0 280.5,0 286.5,0 292.5,-6 292.5,-12 292.5,-12 292.5,-45.5 292.5,-45.5 292.5,-51.5 286.5,-57.5 280.5,-57.5\"/>\n",
       "<text text-anchor=\"start\" x=\"211.38\" y=\"-40.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"197.12\" y=\"-23.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 25]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.39,-93.21C204.8,-84.82 210.69,-75.66 216.28,-66.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.71,-69.11 222.18,-58.81 213.83,-65.33 219.71,-69.11\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x11a667433a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the tree\n",
    "\n",
    "The [source code for Tree](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx) has useful comments about attributes in the Tree class.  Search for the code that says `cdef class Tree:` for useful comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sklearn.tree._tree.Tree object at 0x0000011A05FF3210>\n"
     ]
    }
   ],
   "source": [
    "# get the Tree object\n",
    "tree0 = model.tree_\n",
    "print(tree0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree attributes are stored in lists\n",
    "\n",
    "The tree data are stored in lists.  Each node is also assigned an integer 0,1,2...  \n",
    "Each node's value for some attribute is stored at the index location that equals the node's assigned integer.\n",
    "\n",
    "For example, node 0 is the root node at the top of the tree.  There is a list called children_left.  Index location 0 contains the left child of node 0.\n",
    "\n",
    "\n",
    "#### left and right child nodes\n",
    "```\n",
    "children_left : array of int, shape [node_count]\n",
    "        children_left[i] holds the node id of the left child of node i.\n",
    "        For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
    "        children_left[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] <= threshold[i].\n",
    "children_right : array of int, shape [node_count]\n",
    "        children_right[i] holds the node id of the right child of node i.\n",
    "        For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
    "        children_right[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] > threshold[i].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this tree, the index positions 0,1,2,3,4 are the numbers for identifying each node in the tree.  Node 0 is the root node.  Node 1 and 2 are the left and right child of the root node.  So in the list children_left, at index 0, we see 1, and for children_right list, at index 0, we see 2.  \n",
    "\n",
    "-1 is used to denote that there is no child for that node.  Node 1 has no left or right child, so in the children_left list, at index 1, we see -1.  Similarly, in children_right, at index 1, the value is also -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features used for splitting at each node\n",
    "```\n",
    "    feature : array of int, shape [node_count]\n",
    "        feature[i] holds the feature to split on, for the internal node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature 1 is used to split on node 0.  Feature 0 is used to split on node 2.  The -2 values indicate that these are leaf nodes (no features are used for splitting at those nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of samples in each node\n",
    "\n",
    "```\n",
    "n_node_samples : array of int, shape [node_count]\n",
    "        n_node_samples[i] holds the number of training samples reaching node i.\n",
    "\n",
    "weighted_n_node_samples : array of int, shape [node_count]\n",
    "        weighted_n_node_samples[i] holds the weighted number of training samples\n",
    "        reaching node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.n_node_samples : [100  50  50  25  25]\n",
      "tree0.weighted_n_node_samples : [100.  50.  50.  25.  25.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.n_node_samples : {tree0.n_node_samples}\")\n",
    "print(f\"tree0.weighted_n_node_samples : {tree0.weighted_n_node_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted_n_node_samples is the same as n_node_samples for decision trees.  It's different for random forests where a sub-sample of data points is used in each tree.  We'll use weighted_n_node_samples in the code below, but either one works when we're calculating the proportion of samples in a left or right child node relative to their parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity\n",
    "\n",
    "Gini impurity, like entropy is a way to measure how \"disorganized\" the observations are before and after splitting them using a feature. So there is an impurity value calculated for each node.\n",
    "\n",
    "In the formula, $freq_{i}$ is the frequency of label \"i\".  C is the number of unique labels at that node (C stands for \"Class\", as in \"classifier\".\n",
    "\n",
    "$ \\sum_{i}^{C} - freq_{i} * (1- freq_{i})$\n",
    "\n",
    "```\n",
    "impurity : array of double, shape [node_count]\n",
    "        impurity[i] holds the impurity (i.e., the value of the splitting\n",
    "        criterion) at node i.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impurity if there is a single class (unique label type)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity of a homogenous sample with a single label, is: 0\n"
     ]
    }
   ],
   "source": [
    "freq0 = 1\n",
    "impurity = -1 * freq0 * (1 - freq0)\n",
    "print(f\"impurity of a homogenous sample with a single label, is: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impurity if there are two classes (two distinct labels), and there are 90% of samples for one label, and 10% for the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity when 90% are of one label, and 10% are of the other: -0.18\n"
     ]
    }
   ],
   "source": [
    "freq1 = 0.9\n",
    "freq2 = 0.1\n",
    "impurity = -1 * freq1 * (1 -freq1) + -1 * freq2 * (1 - freq2)\n",
    "print(f\"impurity when 90% are of one label, and 10% are of the other: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "What is the impurity if there are two classes of label, and there are 50% of samples for one label, and 50% for the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity when 50% are of one label, and 50% are of the other: -0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What is the impurity if there are two classes of label,\n",
    "and there are 50% of samples for one label, and 50% for the other?\n",
    "\"\"\"\n",
    "# TODO\n",
    "freq1 = 0.5\n",
    "freq2 = 0.5\n",
    "# TODO\n",
    "impurity = -freq1 * (1-freq1) - freq2 * (1-freq2)\n",
    "print(f\"impurity when 50% are of one label, and 50% are of the other: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Is the impurity larger or smaller (in magnitude) when the samples are dominated by a single class?  Smaller\n",
    "\n",
    "Is the impurity larger or smaller (in magnitude) when the frequency of each class is more evenly distributed among classes? Larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Importance\n",
    "\n",
    "The node importance in sklearn is calculated as the difference between the gini impurity of the node and the impurities of its child nodes.  These gini impurities are weighted by the number of data points that reach each node.\n",
    "\n",
    "$NodeImportance = w_{i} Impurity_{i} - ( w_{left} Impurity_{left} + w_{right} Impurity_{right} )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the node labels\n",
    "Node 0 is the root node, and its left and right children are 1 and 2.  \n",
    "Node 1 is a leaf node  \n",
    "Node 2 has two children, 3 and 4.  \n",
    "Node 3 is a leaf node  \n",
    "Node 4 is a leaf node  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# summary of child nodes\n",
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the node importance of the root node, node 0.  Its child nodes are 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of node 0 is 12.5\n"
     ]
    }
   ],
   "source": [
    "ni0 = tree0.weighted_n_node_samples[0] * tree0.impurity[0] - \\\n",
    "        ( tree0.weighted_n_node_samples[1] * tree0.impurity[1] + \\\n",
    "          tree0.weighted_n_node_samples[2] * tree0.impurity[2] )\n",
    "print(f\"Importance of node 0 is {ni0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Calculate the node importance of node 2.  Its left and right child nodes are 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of node 2 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ni2 = tree0.weighted_n_node_samples[2] * tree0.impurity[2] - \\\n",
    "        ( tree0.weighted_n_node_samples[3] * tree0.impurity[3] + \\\n",
    "          tree0.weighted_n_node_samples[4] * tree0.impurity[4] )\n",
    "print(f\"Importance of node 2 is {ni2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other nodes are leaf nodes, so there is no decrease in impurity that we can calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum the node importances\n",
    "Only nodes 0 and node 2 have node importances.  The others are leaf nodes, so we don't calculate node importances (there is no feature that is used for splitting at those leaf nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of node importances is 37.5\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ni_total = ni0 + ni2\n",
    "print(f\"The sum of node importances is {ni_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of which feature is used to split at each node\n",
    "\n",
    "feature 0 was used to split on node 2  \n",
    "feature 1 was used to split on node 0  \n",
    "feature 2 was not used for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Calculate importance of the features\n",
    "\n",
    "The importance of a feature is the importance of the node that it was used for splitting, divided by the total node importances.  Calculate the importance of feature 0, 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importance of feature 0: 0.6666666666666666\n",
      "importance of feature 1: 0.3333333333333333\n",
      "importance of feature 2: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "fi0 = ni2 / ni_total\n",
    "fi1 = ni0 / ni_total\n",
    "fi2 = 0\n",
    "print(f\"importance of feature 0: {fi0}\")\n",
    "print(f\"importance of feature 1: {fi1}\")\n",
    "print(f\"importance of feature 2: {fi2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check with sklearn\n",
    "\n",
    "Check out how to use [feature importance](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn importance of feature 0: 0.6666666666666666\n",
      "sklearn importance of feature 1: 0.3333333333333333\n",
      "sklearn importance of feature 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: get feature importances from sci-kit learn\n",
    "\n",
    "fi0_skl = model.feature_importances_[0]\n",
    "fi1_skl = model.feature_importances_[1]\n",
    "fi2_skl = model.feature_importances_[2]\n",
    "\n",
    "print(f\"sklearn importance of feature 0: {fi0_skl}\")\n",
    "print(f\"sklearn importance of feature 1: {fi1_skl}\")\n",
    "print(f\"sklearn importance of feature 2: {fi2_skl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice anything odd?\n",
    "\n",
    "Notice that the data we generated simulates an AND operator.  If feature 0 and feature 1 are both 1, then the output is 1, otherwise 0.  So, from that perspective, do you think that features 0 and 1 are equally important?\n",
    "\n",
    "What do you notice about the feature importance calculated in sklearn?  Are the features considered equally important according to this calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "If someone tells you that you don't need to understand the algorithm, just how to install the package and call the function, do you agree or disagree with that statement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution notebook\n",
    "[Solution notebook](sklearn_feature_importance_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
